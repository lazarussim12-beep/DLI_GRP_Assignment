{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUdeEA4LXY9aJYRT5bHyQ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lazarussim12-beep/DLI_GRP_Assignment/blob/Ahmed/Ahmed%20Mohammed%20Khaled%20individual%20code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEz0s7ZCfwZ9"
      },
      "outputs": [],
      "source": [
        "# === REPRODUCIBILITY: Fix all random seeds ===\n",
        "os.environ['PYTHONHASHSEED'] = '42'\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load dataset\n",
        "csv_files = [f for f in os.listdir('enron_data') if f.endswith('.csv')]\n",
        "df = pd.read_csv(f'enron_data/{csv_files[0]}')\n",
        "\n",
        "print(f\"Dataset loaded: {df.shape}\")\n",
        "\n",
        "# Data preprocessing\n",
        "df['Message'] = df['Message'].astype(str).str.lower().apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
        "df['Spam/Ham'] = df['Spam/Ham'].map({'ham': 0, 'spam': 1})\n",
        "X = df['Message'].values\n",
        "y = df['Spam/Ham'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "# Tokenization for neural network\n",
        "max_words = 5000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len)\n",
        "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len)\n",
        "\n",
        "# Fast Neural Network\n",
        "model = Sequential([\n",
        "    Embedding(max_words, 32, input_length=max_len),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "    X_train_seq, y_train,\n",
        "    batch_size=256,\n",
        "    epochs=3,\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "proba_nn = model.predict(X_test_seq, verbose=0).flatten()\n",
        "\n",
        "# TF-IDF features\n",
        "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 3))\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Count Vectorizer for Bernoulli NB\n",
        "count_vectorizer = CountVectorizer(max_features=3000, binary=True)\n",
        "X_train_count = count_vectorizer.fit_transform(X_train)\n",
        "X_test_count = count_vectorizer.transform(X_test)\n",
        "\n",
        "# Extra Trees Classifier\n",
        "et = ExtraTreesClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
        "et.fit(X_train_tfidf, y_train)\n",
        "proba_et = et.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "# SGD Classifier\n",
        "sgd = SGDClassifier(loss='log_loss', random_state=42, max_iter=1000)\n",
        "sgd.fit(X_train_tfidf, y_train)\n",
        "sgd_scores = sgd.decision_function(X_test_tfidf)\n",
        "proba_sgd = 1 / (1 + np.exp(-sgd_scores))\n",
        "\n",
        "# Bernoulli Naive Bayes\n",
        "bnb = BernoulliNB(alpha=1.0)\n",
        "bnb.fit(X_train_count, y_train)\n",
        "proba_bnb = bnb.predict_proba(X_test_count)[:, 1]\n",
        "\n",
        "# Logistic Regression (additional model like your friend's code)\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr.fit(X_train_tfidf, y_train)\n",
        "proba_lr = lr.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "# Function to calculate model parameters count\n",
        "def count_parameters(model, model_type='sklearn'):\n",
        "    if model_type == 'keras':\n",
        "        return model.count_params()\n",
        "    elif hasattr(model, 'coef_'):\n",
        "        return model.coef_.size + 1\n",
        "    elif hasattr(model, 'n_features_in_'):\n",
        "        return model.n_features_in_\n",
        "    else:\n",
        "        return \"N/A\"\n",
        "\n",
        "# Function to measure inference time\n",
        "def measure_inference_time(model, X_test_data, model_type='sklearn'):\n",
        "    start_time_inf = time.time()\n",
        "    if model_type == 'sklearn':\n",
        "        model.predict(X_test_data)\n",
        "    elif model_type == 'keras':\n",
        "        model.predict(X_test_data, verbose=0)\n",
        "    end_time_inf = time.time()\n",
        "    return (end_time_inf - start_time_inf) * 1000\n",
        "\n",
        "# Generate predictions for individual models\n",
        "et_pred = (proba_et > 0.5).astype(int)\n",
        "sgd_pred = (proba_sgd > 0.5).astype(int)\n",
        "bnb_pred = (proba_bnb > 0.5).astype(int)\n",
        "nn_pred = (proba_nn > 0.5).astype(int)\n",
        "lr_pred = (proba_lr > 0.5).astype(int)\n",
        "\n",
        "# Ensemble predictions\n",
        "ensemble_soft_proba = (proba_et + proba_sgd + proba_bnb + proba_nn) / 4\n",
        "ensemble_soft_pred = (ensemble_soft_proba > 0.5).astype(int)\n",
        "\n",
        "# Calculate comprehensive metrics for all models\n",
        "models_results = []\n",
        "\n",
        "# Extra Trees\n",
        "et_acc = accuracy_score(y_test, et_pred)\n",
        "et_prec = precision_score(y_test, et_pred)\n",
        "et_rec = recall_score(y_test, et_pred)\n",
        "et_f1 = f1_score(y_test, et_pred)\n",
        "et_params = count_parameters(et)\n",
        "et_inference = measure_inference_time(et, X_test_tfidf, 'sklearn')\n",
        "models_results.append(['Extra Trees', et_acc, et_prec, et_rec, et_f1, et_params, et_inference])\n",
        "\n",
        "# SGD Classifier\n",
        "sgd_acc = accuracy_score(y_test, sgd_pred)\n",
        "sgd_prec = precision_score(y_test, sgd_pred)\n",
        "sgd_rec = recall_score(y_test, sgd_pred)\n",
        "sgd_f1 = f1_score(y_test, sgd_pred)\n",
        "sgd_params = count_parameters(sgd)\n",
        "sgd_inference = measure_inference_time(sgd, X_test_tfidf, 'sklearn')\n",
        "models_results.append(['SGD Classifier', sgd_acc, sgd_prec, sgd_rec, sgd_f1, sgd_params, sgd_inference])\n",
        "\n",
        "# Bernoulli Naive Bayes\n",
        "bnb_acc = accuracy_score(y_test, bnb_pred)\n",
        "bnb_prec = precision_score(y_test, bnb_pred)\n",
        "bnb_rec = recall_score(y_test, bnb_pred)\n",
        "bnb_f1 = f1_score(y_test, bnb_pred)\n",
        "bnb_params = count_parameters(bnb)\n",
        "bnb_inference = measure_inference_time(bnb, X_test_count, 'sklearn')\n",
        "models_results.append(['Bernoulli Naive Bayes', bnb_acc, bnb_prec, bnb_rec, bnb_f1, bnb_params, bnb_inference])\n",
        "\n",
        "# Fast Neural Network\n",
        "nn_acc = accuracy_score(y_test, nn_pred)\n",
        "nn_prec = precision_score(y_test, nn_pred)\n",
        "nn_rec = recall_score(y_test, nn_pred)\n",
        "nn_f1 = f1_score(y_test, nn_pred)\n",
        "nn_params = count_parameters(model, 'keras')\n",
        "nn_inference = measure_inference_time(model, X_test_seq, 'keras')\n",
        "models_results.append(['Fast Neural Network', nn_acc, nn_prec, nn_rec, nn_f1, nn_params, nn_inference])\n",
        "\n",
        "# Logistic Regression\n",
        "lr_acc = accuracy_score(y_test, lr_pred)\n",
        "lr_prec = precision_score(y_test, lr_pred)\n",
        "lr_rec = recall_score(y_test, lr_pred)\n",
        "lr_f1 = f1_score(y_test, lr_pred)\n",
        "lr_params = count_parameters(lr)\n",
        "lr_inference = measure_inference_time(lr, X_test_tfidf, 'sklearn')\n",
        "models_results.append(['Logistic Regression (TF-IDF)', lr_acc, lr_prec, lr_rec, lr_f1, lr_params, lr_inference])\n",
        "\n",
        "# Ensemble\n",
        "ensemble_acc = accuracy_score(y_test, ensemble_soft_pred)\n",
        "ensemble_prec = precision_score(y_test, ensemble_soft_pred)\n",
        "ensemble_rec = recall_score(y_test, ensemble_soft_pred)\n",
        "ensemble_f1 = f1_score(y_test, ensemble_soft_pred)\n",
        "ensemble_params = \"Combined\"\n",
        "ensemble_inference = et_inference + sgd_inference + bnb_inference + nn_inference\n",
        "models_results.append(['Ensemble (ET+SGD+BNB+FastNN)', ensemble_acc, ensemble_prec, ensemble_rec, ensemble_f1, ensemble_params, ensemble_inference])\n",
        "\n",
        "# Print comprehensive results table (like your friend's format)\n",
        "print(\"\\n\" + \"=\"*120)\n",
        "print(\"MODEL EVALUATION TABLE: Accuracy, Precision, Recall, F1, Params, Inference Time\")\n",
        "print(\"=\"*120)\n",
        "print(f\"{'Model':<35} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'Params':<10} {'Inference (ms)':<15}\")\n",
        "print(\"-\"*120)\n",
        "\n",
        "for model_name, acc, prec, rec, f1, params, inference in models_results:\n",
        "    print(f\"{model_name:<35} {acc:<10.2f} {prec:<10.2f} {rec:<10.2f} {f1:<10.2f} {str(params):<10} {inference:<15.2f}\")\n",
        "\n",
        "print(\"-\"*120)\n",
        "print(\"TF-IDF: Term Frequency-Inverse Document Frequency with N-grams (1-3)\")\n",
        "print(\"ET: Extra Trees Classifier\")\n",
        "print(\"SGD: Stochastic Gradient Descent Classifier\")\n",
        "print(\"BNB: Bernoulli Naive Bayes\")\n",
        "print(\"FastNN: Fast Neural Network with Global Average Pooling\")\n",
        "print(\"LR: Logistic Regression on TF-IDF features\")\n",
        "print(\"=\"*120)\n",
        "\n",
        "time_taken = time.time() - start_time\n",
        "minutes = int(time_taken // 60)\n",
        "seconds = time_taken % 60\n",
        "print(f\"\\nTotal runtime: {time_taken:.2f} seconds ({minutes} minutes {seconds:.2f} seconds)\")\n",
        "\n",
        "# Individual model performance\n",
        "print(f\"\\nIndividual Model Accuracies:\")\n",
        "print(f\"Extra Trees: {et_acc:.2f}\")\n",
        "print(f\"SGD Classifier: {sgd_acc:.2f}\")\n",
        "print(f\"Bernoulli NB: {bnb_acc:.2f}\")\n",
        "print(f\"Fast Neural Net: {nn_acc:.2f}\")\n",
        "print(f\"Logistic Regression: {lr_acc:.2f}\")\n",
        "print(f\"ENSEMBLE: {ensemble_acc:.2f}\")\n",
        "\n",
        "# Final Accuracy Display\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL ENSEMBLE ACCURACY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {ensemble_acc:.2%}\")\n",
        "print(f\"Accuracy: {ensemble_acc:.4f}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# === STORE RESULTS FOR AHMED CODE ===\n",
        "y_test_ahmed = y_test\n",
        "y_pred_ahmed = ensemble_soft_pred\n",
        "y_proba_ahmed = ensemble_soft_proba\n",
        "model_name_ahmed = \"Ahmed_Ensemble\"\n",
        "\n",
        "def store_results(name, y_true, y_pred, y_proba, file_path=\"all_models_results.pkl\"):\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "    results = {\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Precision\": precision_score(y_true, y_pred),\n",
        "        \"Recall\": recall_score(y_true, y_pred),\n",
        "        \"F1\": f1_score(y_true, y_pred),\n",
        "        \"ROC_AUC\": roc_auc_score(y_true, y_proba),\n",
        "        \"y_true\": y_true,\n",
        "        \"y_pred\": y_pred,\n",
        "        \"y_proba\": y_proba\n",
        "    }\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            all_results = pickle.load(f)\n",
        "    else:\n",
        "        all_results = []\n",
        "    all_results.append(results)\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        pickle.dump(all_results, f)\n",
        "    print(f\"✅ Stored results for {name}\")\n",
        "\n",
        "store_results(model_name_ahmed, y_test_ahmed, y_pred_ahmed, y_proba_ahmed)\n",
        "\n",
        "# ======== VISUALIZATION: Confusion Matrix, ROC, and Precision-Recall Curves =========\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, precision_recall_curve, auc\n",
        "\n",
        "# Prepare data for ensemble visualization\n",
        "y_true_ensemble = y_test\n",
        "y_pred_ensemble = ensemble_soft_pred\n",
        "y_proba_ensemble = ensemble_soft_proba\n",
        "\n",
        "# 1. Confusion Matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "cm = confusion_matrix(y_true_ensemble, y_pred_ensemble)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Ham', 'Spam'])\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title('Confusion Matrix - Ensemble Model')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. ROC Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "fpr, tpr, _ = roc_curve(y_true_ensemble, y_proba_ensemble)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) - Ensemble Model')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "precision, recall, _ = precision_recall_curve(y_true_ensemble, y_proba_ensemble)\n",
        "plt.plot(recall, precision, color='blue', lw=2)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Ensemble Model')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}